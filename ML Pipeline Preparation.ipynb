{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///Messages.db')\n",
    "df = pd.read_sql(\"SELECT * FROM Messages\", engine)\n",
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "        def tokenize(text):\n",
    "            \"\"\"\n",
    "            input:\n",
    "            message\n",
    "\n",
    "            Returning:\n",
    "            list of words into numbers of same meaning\n",
    "            \"\"\"\n",
    "            # Converting everything to lower case\n",
    "            text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "            # Tokenize words\n",
    "            tokens = word_tokenize(text)\n",
    "\n",
    "            # normalization word tokens and remove stop words\n",
    "            normalize = PorterStemmer()\n",
    "            stop_words = stopwords.words(\"english\")\n",
    "\n",
    "            normalized = [normalize.stem(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "            return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(ArrayL, ArrayP, col_names):\n",
    "    \"\"\"Evalute metrics of the ML pipeline model\n",
    "    \n",
    "    inputs:\n",
    "    ArrayL: array. Array containing the real labels.\n",
    "    ArrayP: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the ArrayP fields.\n",
    "       \n",
    "    Returns:\n",
    "    data_metrics: Contains accuracy, precision, recall \n",
    "    and f1 score for a given set of ArrayL and ArrayP labels.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Evaluate metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        precision = precision_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        recall = recall_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        f1 = f1_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # store metrics\n",
    "    metrics = np.array(metrics)\n",
    "    data_metrics = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return data_metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.983532   0.982047  0.996584  0.989262\n",
      "request                 0.975834   0.990866  0.867595  0.925142\n",
      "offer                   0.998419   0.983607  0.666667  0.794702\n",
      "aid_related             0.969766   0.991407  0.935274  0.962523\n",
      "medical_help            0.979402   0.993186  0.746479  0.852339\n",
      "medical_products        0.984960   0.992733  0.701953  0.822396\n",
      "search_and_rescue       0.990976   1.000000  0.683929  0.812301\n",
      "security                0.994953   1.000000  0.725000  0.840580\n",
      "military                0.991231   1.000000  0.734568  0.846975\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.983481   0.992325  0.740589  0.848172\n",
      "food                    0.975987   0.995319  0.786044  0.878389\n",
      "shelter                 0.978026   0.993289  0.759407  0.860743\n",
      "clothing                0.995615   0.991071  0.725490  0.837736\n",
      "money                   0.994086   0.993921  0.741497  0.849351\n",
      "missing_people          0.996941   0.994253  0.745690  0.852217\n",
      "refugees                0.990058   1.000000  0.705882  0.827586\n",
      "death                   0.987611   0.997054  0.737473  0.847840\n",
      "other_aid               0.970990   0.993646  0.785245  0.877238\n",
      "infrastructure_related  0.984399   0.993782  0.761716  0.862410\n",
      "transport               0.987713   1.000000  0.731327  0.844816\n",
      "buildings               0.987050   0.990728  0.751759  0.854857\n",
      "electricity             0.993831   1.000000  0.697500  0.821797\n",
      "tools                   0.998827   1.000000  0.803419  0.890995\n",
      "hospitals               0.996890   1.000000  0.708134  0.829132\n",
      "shops                   0.998776   1.000000  0.720930  0.837838\n",
      "aid_centers             0.996380   1.000000  0.677273  0.807588\n",
      "other_infrastructure    0.988478   0.998413  0.736534  0.847709\n",
      "weather_related         0.968288   0.992689  0.892948  0.940181\n",
      "floods                  0.981136   0.994427  0.774814  0.870990\n",
      "storm                   0.980626   0.994624  0.799136  0.886228\n",
      "fire                    0.996839   1.000000  0.721973  0.838542\n",
      "earthquake              0.979861   0.992527  0.791870  0.880916\n",
      "cold                    0.993678   0.996364  0.690176  0.815476\n",
      "other_weather           0.986846   0.994937  0.755769  0.859016\n",
      "direct_report           0.972418   0.993636  0.863122  0.923792\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for training set\n",
    "Y_train_pred = pipeline.predict(X_train)\n",
    "col_names = list(Y.columns.values)\n",
    "\n",
    "print(eval_metrics(np.array(y_train), Y_train_pred, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.712035   0.760260  0.902375  0.825244\n",
      "request                 0.822603   0.354167  0.077982  0.127820\n",
      "offer                   0.995412   0.000000  0.000000  0.000000\n",
      "aid_related             0.558036   0.430007  0.222263  0.293053\n",
      "medical_help            0.914666   0.069767  0.005758  0.010638\n",
      "medical_products        0.947698   0.000000  0.000000  0.000000\n",
      "search_and_rescue       0.971861   0.250000  0.011111  0.021277\n",
      "security                0.983178   0.000000  0.000000  0.000000\n",
      "military                0.963450   0.000000  0.000000  0.000000\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.931641   0.031250  0.002398  0.004454\n",
      "food                    0.882398   0.187500  0.020862  0.037547\n",
      "shelter                 0.903961   0.081633  0.006814  0.012579\n",
      "clothing                0.983790   0.000000  0.000000  0.000000\n",
      "money                   0.977519   0.000000  0.000000  0.000000\n",
      "missing_people          0.987307   0.000000  0.000000  0.000000\n",
      "refugees                0.962838   0.000000  0.000000  0.000000\n",
      "death                   0.953510   0.142857  0.006803  0.012987\n",
      "other_aid               0.854718   0.128788  0.019953  0.034553\n",
      "infrastructure_related  0.929194   0.157895  0.006667  0.012793\n",
      "transport               0.953357   0.000000  0.000000  0.000000\n",
      "buildings               0.942346   0.181818  0.011019  0.020779\n",
      "electricity             0.980884   0.000000  0.000000  0.000000\n",
      "tools                   0.993883   0.000000  0.000000  0.000000\n",
      "hospitals               0.989295   0.000000  0.000000  0.000000\n",
      "shops                   0.995259   0.000000  0.000000  0.000000\n",
      "aid_centers             0.987154   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.952286   0.105263  0.006734  0.012658\n",
      "weather_related         0.720752   0.489329  0.177152  0.260130\n",
      "floods                  0.909772   0.098765  0.015238  0.026403\n",
      "storm                   0.902126   0.405797  0.044657  0.080460\n",
      "fire                    0.989907   0.000000  0.000000  0.000000\n",
      "earthquake              0.912372   0.605096  0.156766  0.249017\n",
      "cold                    0.979355   0.100000  0.007874  0.014599\n",
      "other_weather           0.945099   0.157895  0.008671  0.016438\n",
      "direct_report           0.796605   0.368217  0.075277  0.125000\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "Y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "eval_metrics_test = eval_metrics(np.array(y_test), Y_test_pred, col_names)\n",
    "print(eval_metrics_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metric for use in grid search scoring object\n",
    "def perform_metric(Y_T, Y_P):\n",
    "    \"\"\"Median F1 score for all classifiers\n",
    "    \n",
    "    inputs:\n",
    "    Y_T: array. Array containing ArrayL labels.\n",
    "    Y_P: array. Array containing ArrayP labels.\n",
    "        \n",
    "    Routputs:\n",
    "    Median F1 score for all  classifiers\n",
    "    \"\"\"\n",
    "    f1_list = []\n",
    "    for i in range(np.shape(Y_P)[1]):\n",
    "        f1 = f1_score(np.array(Y_T)[:, i], Y_P[:, i])\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "    score = np.median(f1_list)\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=1, score=0.008715662825251866, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=1, score=0.0029673590504451035, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=1, score=0.008108108108108109, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  4.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=3, score=0.009705771761609326, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  6.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=3, score=0.01559957050533154, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  7.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=True, vect__min_df=3, score=0.00814280980781975, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  8.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=1, score=0.009501241254795758, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 10.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=1, score=0.004462242562929061, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 12.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=1, score=0.009684437062855272, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 13.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=3, score=0.00781875453599994, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=3 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=3, score=0.010775768299226987, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=3 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=12, tfidf__use_idf=False, vect__min_df=3, score=0.011510315458717491, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=1, score=0.009489930317119023, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=1, score=0.0058576480990274096, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=1, score=0.00851063829787234, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=3 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=3, score=0.007716049382716051, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=3 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=3, score=0.014413265306122448, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=3 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=True, vect__min_df=3, score=0.007213625453245493, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=1, score=0.007560165193645156, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=1, score=0.002347417840375587, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=1, score=0.0057165723432230035, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=3 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=3, score=0.006108023424969226, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=3 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=3, score=0.016830592493508538, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=3 \n",
      "[CV]  clf__estimator__min_samples_split=6, clf__estimator__n_estimators=14, tfidf__use_idf=False, vect__min_df=3, score=0.011403551499997293, total= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 37.1min finished\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "\n",
    "parameters = {'vect__min_df': [1, 3],\n",
    "              'tfidf__use_idf':[True, False],\n",
    "              'clf__estimator__n_estimators':[12, 14], \n",
    "              'clf__estimator__min_samples_split':[6]}\n",
    "\n",
    "scorer = make_scorer(perform_metric)\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = scorer, verbose = 10)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(74)\n",
    "T_model = cv.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 60.51885684,  57.8440481 ,  59.76982848,  55.0184509 ,\n",
       "         67.70449011,  65.15158359,  67.44481738,  61.39497828]),\n",
       " 'std_fit_time': array([ 0.25335554,  0.68522454,  0.16463509,  0.31004568,  0.22116516,\n",
       "         0.43850717,  0.48501612,  0.53449203]),\n",
       " 'mean_score_time': array([ 10.27765028,   9.89492496,  10.18499796,   9.81885568,\n",
       "         10.83773311,  10.33359281,  10.78072095,  10.1548423 ]),\n",
       " 'std_score_time': array([ 0.12937303,  0.03047195,  0.04424241,  0.068262  ,  0.09308761,\n",
       "         0.07219223,  0.03899409,  0.08299798]),\n",
       " 'param_clf__estimator__min_samples_split': masked_array(data = [6 6 6 6 6 6 6 6],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [12 12 12 12 14 14 14 14],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__min_df': masked_array(data = [1 3 1 3 1 3 1 3],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__min_samples_split': 6,\n",
       "   'clf__estimator__n_estimators': 12,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 6,\n",
       "   'clf__estimator__n_estimators': 12,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 3},\n",
       "  {'clf__estimator__min_samples_split': 6,\n",
       "   'clf__estimator__n_estimators': 12,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 6,\n",
       "   'clf__estimator__n_estimators': 12,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 3},\n",
       "  {'clf__estimator__min_samples_split': 6,\n",
       "   'clf__estimator__n_estimators': 14,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 6,\n",
       "   'clf__estimator__n_estimators': 14,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 3},\n",
       "  {'clf__estimator__min_samples_split': 6,\n",
       "   'clf__estimator__n_estimators': 14,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 6,\n",
       "   'clf__estimator__n_estimators': 14,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 3}],\n",
       " 'split0_test_score': array([ 0.00871566,  0.00970577,  0.00950124,  0.00781875,  0.00948993,\n",
       "         0.00771605,  0.00756017,  0.00610802]),\n",
       " 'split1_test_score': array([ 0.00296736,  0.01559957,  0.00446224,  0.01077577,  0.00585765,\n",
       "         0.01441327,  0.00234742,  0.01683059]),\n",
       " 'split2_test_score': array([ 0.00810811,  0.00814281,  0.00968444,  0.01151032,  0.00851064,\n",
       "         0.00721363,  0.00571657,  0.01140355]),\n",
       " 'mean_test_score': array([ 0.00659704,  0.01114938,  0.00788264,  0.01003495,  0.00795274,\n",
       "         0.00978098,  0.00520805,  0.01144739]),\n",
       " 'std_test_score': array([ 0.00257853,  0.0032108 ,  0.00241974,  0.00159552,  0.00153445,\n",
       "         0.00328194,  0.00215826,  0.00437758]),\n",
       " 'rank_test_score': array([7, 2, 6, 3, 5, 4, 8, 1], dtype=int32),\n",
       " 'split0_train_score': array([ 0.71582415,  0.64303017,  0.69371566,  0.59819867,  0.72928308,\n",
       "         0.63015721,  0.68849965,  0.61988346]),\n",
       " 'split1_train_score': array([ 0.7134371 ,  0.64255993,  0.69687559,  0.61397644,  0.73436187,\n",
       "         0.65826259,  0.70453317,  0.60891178]),\n",
       " 'split2_train_score': array([ 0.71705681,  0.64127602,  0.69593122,  0.61944444,  0.73071257,\n",
       "         0.64410601,  0.71052755,  0.6222987 ]),\n",
       " 'mean_train_score': array([ 0.71543935,  0.6422887 ,  0.69550749,  0.61053985,  0.73145251,\n",
       "         0.64417527,  0.70118679,  0.61703131]),\n",
       " 'std_train_score': array([ 0.00150258,  0.00074137,  0.00132437,  0.00900753,  0.0021384 ,\n",
       "         0.01147408,  0.00929895,  0.00582543])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search results\n",
    "T_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011447389139491686"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top mean score\n",
    "np.max(T_model.cv_results_['mean_test_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_split': 6,\n",
       " 'clf__estimator__n_estimators': 14,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__min_df': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for top mean score\n",
    "T_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.737116   0.767000  0.940337  0.844870\n",
      "request                 0.814039   0.406639  0.083689  0.138810\n",
      "offer                   0.996024   0.000000  0.000000  0.000000\n",
      "aid_related             0.549319   0.433550  0.269655  0.332503\n",
      "medical_help            0.913289   0.092308  0.011673  0.020725\n",
      "medical_products        0.946628   0.057143  0.006289  0.011331\n",
      "search_and_rescue       0.969720   0.040000  0.005714  0.010000\n",
      "security                0.980425   0.000000  0.000000  0.000000\n",
      "military                0.963603   0.045455  0.004587  0.008333\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.927206   0.075000  0.006787  0.012448\n",
      "food                    0.877198   0.178082  0.017196  0.031363\n",
      "shelter                 0.899526   0.112676  0.013289  0.023774\n",
      "clothing                0.983331   0.142857  0.009615  0.018018\n",
      "money                   0.976143   0.000000  0.000000  0.000000\n",
      "missing_people          0.986389   0.000000  0.000000  0.000000\n",
      "refugees                0.961615   0.000000  0.000000  0.000000\n",
      "death                   0.949992   0.000000  0.000000  0.000000\n",
      "other_aid               0.851965   0.111888  0.018670  0.032000\n",
      "infrastructure_related  0.927971   0.071429  0.006897  0.012579\n",
      "transport               0.948922   0.076923  0.006410  0.011834\n",
      "buildings               0.944334   0.062500  0.005952  0.010870\n",
      "electricity             0.978896   0.000000  0.000000  0.000000\n",
      "tools                   0.994495   0.000000  0.000000  0.000000\n",
      "hospitals               0.989448   0.000000  0.000000  0.000000\n",
      "shops                   0.996636   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988224   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.949686   0.030303  0.003356  0.006042\n",
      "weather_related         0.723811   0.535294  0.196332  0.287293\n",
      "floods                  0.910384   0.190476  0.014286  0.026578\n",
      "storm                   0.897997   0.352381  0.058176  0.099865\n",
      "fire                    0.989448   0.000000  0.000000  0.000000\n",
      "earthquake              0.911913   0.581152  0.182867  0.278195\n",
      "cold                    0.979355   0.000000  0.000000  0.000000\n",
      "other_weather           0.944946   0.263158  0.014245  0.027027\n",
      "direct_report           0.789111   0.326923  0.065943  0.109748\n"
     ]
    }
   ],
   "source": [
    "# evaluating metrics for test set\n",
    "tuned_pred_test_1 = T_model.predict(X_test)\n",
    "\n",
    "eval_metrics_test_1 = eval_metrics(np.array(y_test), tuned_pred_test_1, col_names)\n",
    "\n",
    "print(eval_metrics_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.920808</td>\n",
       "      <td>0.137587</td>\n",
       "      <td>0.053943</td>\n",
       "      <td>0.065395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.095037</td>\n",
       "      <td>0.195320</td>\n",
       "      <td>0.163938</td>\n",
       "      <td>0.158321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.549319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.907669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.949304</td>\n",
       "      <td>0.059821</td>\n",
       "      <td>0.006121</td>\n",
       "      <td>0.011101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.981152</td>\n",
       "      <td>0.181181</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>0.028111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.940337</td>\n",
       "      <td>0.844870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  36.000000  36.000000  36.000000  36.000000\n",
       "mean    0.920808   0.137587   0.053943   0.065395\n",
       "std     0.095037   0.195320   0.163938   0.158321\n",
       "min     0.549319   0.000000   0.000000   0.000000\n",
       "25%     0.907669   0.000000   0.000000   0.000000\n",
       "50%     0.949304   0.059821   0.006121   0.011101\n",
       "75%     0.981152   0.181181   0.015013   0.028111\n",
       "max     1.000000   0.767000   0.940337   0.844870"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of 1st model\n",
    "eval_metrics_test_1.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] clf__estimator__criterion=gini, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__criterion=gini, tfidf__use_idf=True, vect__min_df=1, score=0.05128205128205128, total= 6.0min\n",
      "[CV] clf__estimator__criterion=gini, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__criterion=gini, tfidf__use_idf=True, vect__min_df=1, score=0.05402424159184163, total= 6.1min\n",
      "[CV] clf__estimator__criterion=gini, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 12.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__criterion=gini, tfidf__use_idf=True, vect__min_df=1, score=0.049869109947643986, total= 6.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 19.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 19.1min finished\n"
     ]
    }
   ],
   "source": [
    "# testing a pure decision tree classifier\n",
    "moc = MultiOutputClassifier(DecisionTreeClassifier())\n",
    "\n",
    "pipeline_3 = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', moc)\n",
    "    ])\n",
    "\n",
    "parameters_2 = {'vect__min_df': [1],\n",
    "              'tfidf__use_idf':[True],\n",
    "              'clf__estimator__criterion': ['gini'], \n",
    "                }\n",
    "\n",
    "cv2 = GridSearchCV(pipeline_3, param_grid = parameters_2, scoring = scorer, verbose = 10)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(25)\n",
    "T_model2 = cv2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f5770a61620>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f5770a61620>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': None,\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__presort': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__splitter': 'best',\n",
       " 'clf__estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_3.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.664475   0.768208  0.800924  0.784225\n",
      "request                 0.776265   0.344350  0.275833  0.306306\n",
      "offer                   0.993118   0.000000  0.000000  0.000000\n",
      "aid_related             0.528368   0.433309  0.432035  0.432671\n",
      "medical_help            0.867870   0.083333  0.068093  0.074946\n",
      "medical_products        0.917419   0.048780  0.037736  0.042553\n",
      "search_and_rescue       0.949686   0.047059  0.045714  0.046377\n",
      "security                0.967732   0.020619  0.016949  0.018605\n",
      "military                0.941275   0.048913  0.041284  0.044776\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.892338   0.110119  0.083710  0.095116\n",
      "food                    0.822144   0.138544  0.103175  0.118271\n",
      "shelter                 0.845236   0.091633  0.076412  0.083333\n",
      "clothing                0.972932   0.060241  0.048077  0.053476\n",
      "money                   0.959933   0.034783  0.025806  0.029630\n",
      "missing_people          0.979355   0.000000  0.000000  0.000000\n",
      "refugees                0.941887   0.031250  0.021739  0.025641\n",
      "death                   0.917571   0.042636  0.036304  0.039216\n",
      "other_aid               0.786665   0.151554  0.136523  0.143646\n",
      "infrastructure_related  0.886986   0.068182  0.055172  0.060991\n",
      "transport               0.920171   0.055085  0.041667  0.047445\n",
      "buildings               0.915889   0.082031  0.062500  0.070946\n",
      "electricity             0.965897   0.029703  0.023438  0.026201\n",
      "tools                   0.988836   0.000000  0.000000  0.000000\n",
      "hospitals               0.979966   0.000000  0.000000  0.000000\n",
      "shops                   0.993730   0.000000  0.000000  0.000000\n",
      "aid_centers             0.979202   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.921242   0.030303  0.023490  0.026465\n",
      "weather_related         0.641382   0.363989  0.354369  0.359115\n",
      "floods                  0.849977   0.132635  0.135714  0.134157\n",
      "storm                   0.860070   0.225933  0.180818  0.200873\n",
      "fire                    0.981496   0.018519  0.014493  0.016260\n",
      "earthquake              0.869399   0.261122  0.222405  0.240214\n",
      "cold                    0.962685   0.008403  0.007874  0.008130\n",
      "other_weather           0.906714   0.048780  0.039886  0.043887\n",
      "direct_report           0.732681   0.283695  0.233514  0.256170\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "tuned_pred_test_2 = T_model2.predict(X_test)\n",
    "\n",
    "eval_metrics_test_2 = eval_metrics(np.array(y_test), tuned_pred_test_2, col_names)\n",
    "\n",
    "print(eval_metrics_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle best model\n",
    "pickle.dump(T_model, open('disaster_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
